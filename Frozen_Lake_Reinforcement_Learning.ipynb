{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozen Lake - Reinforcement Learning",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdCEGHQpcCx"
      },
      "source": [
        "# Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5VGvOVvt4I6"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4cifRquqQ6O"
      },
      "source": [
        "![FrozenLake](https://deeplizard.com/images/frozen%20lake%20winter.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjLgBcYpw6p"
      },
      "source": [
        "In this Notebook, we will try to solve the famous Frozen Lake Problem. It is considered as a **Hello World** problem in Reinforcement Learning. We use Open AI Gym and Python to solve this Problem. Anyone with basic knowledge of Python and Reinforcement Learning can easily follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW48Hi1BqPPW"
      },
      "source": [
        "![OpenAI](https://deeplizard.com/images/OpenAI%20logo.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QzUHEguqGqO"
      },
      "source": [
        "**Description of the Problem**\n",
        "\n",
        "_Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend._ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIRzVFbrOVw"
      },
      "source": [
        "Let's have a look at our environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ShiqhAolpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ad3a7b-3e97-434a-dd5f-d2355cb401c6"
      },
      "source": [
        "# Simple Python Code to demomstrate the Environment\n",
        "from tabulate import tabulate\n",
        "environment = [['S','F','F','F'],\n",
        "               ['F','H','F','H'],\n",
        "               ['F','F','F','H'],\n",
        "               ['H','F','F','G']]\n",
        "print(tabulate(environment,tablefmt=\"fancy_grid\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "╒═══╤═══╤═══╤═══╕\n",
            "│ S │ F │ F │ F │\n",
            "├───┼───┼───┼───┤\n",
            "│ F │ H │ F │ H │\n",
            "├───┼───┼───┼───┤\n",
            "│ F │ F │ F │ H │\n",
            "├───┼───┼───┼───┤\n",
            "│ H │ F │ F │ G │\n",
            "╘═══╧═══╧═══╧═══╛\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQFtpbqqs9hC"
      },
      "source": [
        "S --> Starting position of the Agent (Safe, reward = 0)\n",
        "\n",
        "F --> Frozen Part of Lake (Safe, reward = 0)\n",
        "\n",
        "H --> Hole (Unsafe, terminates the episode, reward = 0)\n",
        "\n",
        "G --> Goal (Safe, terminates the episode, reward = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSmaJjwrt7Fv"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhYU0fRt9Um"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOCQkZupecE"
      },
      "source": [
        "# for setting up our environment\n",
        "import gym   \n",
        "\n",
        "# for building our policy\n",
        "import random\n",
        "import time\n",
        "\n",
        "# for numerical computation\n",
        "import numpy as np\n",
        "\n",
        "# optional package, for better presentation\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj6_f624uBBt"
      },
      "source": [
        "# initialize the environment object (Frozen Lake)\n",
        "lake = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7DaoriBaoho",
        "outputId": "03f9d39f-96ce-47a5-f69c-e953d4a456a8"
      },
      "source": [
        "# The Frozen Lake Environment by OPEN AI\n",
        "# Red Spot denotes the position of the Agent\n",
        "lake.render()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqrimlKbvsA",
        "outputId": "ba29a33c-de61-4b10-cc50-bc3eab86b754"
      },
      "source": [
        "print(\" The number of possible actions in the environment are {}\".format(lake.action_space.n))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The number of possible actions in the environment are 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFwj7jCmb-zZ",
        "outputId": "c4fef90b-4315-4dcd-a4d1-555ee22dc1fa"
      },
      "source": [
        "print(\" The number of possible states in the environment are {}\".format(lake.observation_space.n))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The number of possible states in the environment are 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNdHcOa1TOVd"
      },
      "source": [
        "### Create the Q Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTbdQgNDwCLV"
      },
      "source": [
        "# Get the values for columns and rows for the Q Table\n",
        "columns = lake.action_space.n\n",
        "rows = lake.observation_space.n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJSuWO1lTnuv"
      },
      "source": [
        "# Initialize the Q Table with zero as the default value\n",
        "q_table = np.zeros((rows, columns))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9GkIPfpT7jk"
      },
      "source": [
        "# Intializing the parameters\n",
        "episodes = 10000\n",
        "# Max possible steps in an episode\n",
        "steps_per_episode = 100\n",
        "\n",
        "# define the learning rate\n",
        "alpha = 0.1\n",
        "\n",
        "# define the discount rate\n",
        "gamma = 0.99\n",
        "\n",
        "# handling the exploration-exploitation problem\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdw2ka6cdN-y"
      },
      "source": [
        "### Value Iteration Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRGxYYSgVtwY"
      },
      "source": [
        "# list to store return of each episode\n",
        "returns = list()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNEQi0pSbGc2"
      },
      "source": [
        "# Define the Policy for our agent\n",
        "def policy():\n",
        "  # threshold for exploration rate\n",
        "  threshold = random.uniform(0, 1)\n",
        "\n",
        "  if threshold > exploration_rate:\n",
        "    # exploit    \n",
        "    action = np.argmax(q_table[state,:])    \n",
        "  else:\n",
        "    # explore  \n",
        "    action = lake.action_space.sample()\n",
        "  \n",
        "  return action\n",
        "    "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3AUWBd3iB0i"
      },
      "source": [
        "# outer \"for loop\" to control number of episodes\n",
        "# and reset the environment after completing each episode\n",
        "for episode in range(episodes):\n",
        "\n",
        "  #resetting the environment for new episode\n",
        "  state = lake.reset()\n",
        "\n",
        "  # to keep track if the episode is finished or not\n",
        "  flag = False\n",
        "\n",
        "  # setting value of reward to zero for new episode\n",
        "  current_reward = 0\n",
        "\n",
        "  # The inner \"for loop\" is responsible to govern an individual episode\n",
        "  for step in range(steps_per_episode):\n",
        "\n",
        "    action = policy()\n",
        "\n",
        "    # taking action\n",
        "    new_state, reward, flag, info = lake.step(action)\n",
        "\n",
        "    # update Q Table\n",
        "    q_table[state, action] = (q_table[state, action] * (1 - alpha)) + \\\n",
        "    alpha * (reward + gamma * np.max(q_table[new_state, :]))\n",
        "\n",
        "    # transition to the new state\n",
        "    state = new_state\n",
        "\n",
        "    # Adding reward for the current action to the reward for the episode\n",
        "    current_reward += reward\n",
        "\n",
        "    # check if the episode is completed, and start new episode if True\n",
        "    if flag==True:\n",
        "      break\n",
        "\n",
        "  # we reduce the exploration rate as our agent \n",
        "  #learns more and more about the environment\n",
        "  exploration_rate = min_exploration_rate + \\\n",
        "  (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "  # add the current episode reward to the return\n",
        "  returns.append(current_reward)    "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQru1_R7dUbv"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEaiDHXsWvsD",
        "outputId": "6a69942b-5f97-490f-d249-5fb21d5d307d"
      },
      "source": [
        "# Let's have a look at our final Q Table\n",
        "print(q_table)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.50436459 0.49067589 0.49070627 0.49024687]\n",
            " [0.38060542 0.41066292 0.35745717 0.47809896]\n",
            " [0.44020106 0.3951335  0.40622451 0.45688504]\n",
            " [0.28864601 0.37323245 0.15343251 0.44993588]\n",
            " [0.52693571 0.29136583 0.40151514 0.26030547]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21250679 0.15162057 0.30840944 0.09645839]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.38745849 0.45413858 0.28094238 0.54779822]\n",
            " [0.36735147 0.56840381 0.49358777 0.36823383]\n",
            " [0.52936827 0.43043111 0.43100608 0.2847426 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.48221991 0.56164338 0.67940278 0.51543204]\n",
            " [0.70496801 0.86302654 0.77681041 0.72394217]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHeynSXUwKPR"
      },
      "source": [
        "The problem with the Frozen Lake Environment is that the probability of the agent taking the action that it actually want to take is less. This is because the lake is \"slippery\"! To counter this, we make the following changes. These changes are purely based on the previous performance of the agent and these changes help the agent to perform better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2DqV7Kzskgx"
      },
      "source": [
        "q_table[0,np.argmax(q_table[0])], q_table[0,1] = q_table[0,1], q_table[0,np.argmax(q_table[0])]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzItxQ_AtIkF"
      },
      "source": [
        "q_table[8,np.argmax(q_table[8])], q_table[8,0] = q_table[8,0], q_table[8,np.argmax(q_table[8])]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv0SrM8CtAFR",
        "outputId": "c940da8c-9c3f-422d-89e6-ed2e6d25be54"
      },
      "source": [
        "q_table"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49067589, 0.50436459, 0.49070627, 0.49024687],\n",
              "       [0.38060542, 0.41066292, 0.35745717, 0.47809896],\n",
              "       [0.44020106, 0.3951335 , 0.40622451, 0.45688504],\n",
              "       [0.28864601, 0.37323245, 0.15343251, 0.44993588],\n",
              "       [0.52693571, 0.29136583, 0.40151514, 0.26030547],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.21250679, 0.15162057, 0.30840944, 0.09645839],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.54779822, 0.45413858, 0.28094238, 0.38745849],\n",
              "       [0.36735147, 0.56840381, 0.49358777, 0.36823383],\n",
              "       [0.52936827, 0.43043111, 0.43100608, 0.2847426 ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.48221991, 0.56164338, 0.67940278, 0.51543204],\n",
              "       [0.70496801, 0.86302654, 0.77681041, 0.72394217],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxbQD7SQXWJH",
        "outputId": "c3175d47-261a-4525-c584-7b18b84797f9"
      },
      "source": [
        "# Let's calculate the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(returns),episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"Average reward per thousand episodes\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average reward per thousand episodes\n",
            "\n",
            "1000 :  0.05100000000000004\n",
            "2000 :  0.20200000000000015\n",
            "3000 :  0.4140000000000003\n",
            "4000 :  0.5710000000000004\n",
            "5000 :  0.6270000000000004\n",
            "6000 :  0.6540000000000005\n",
            "7000 :  0.6710000000000005\n",
            "8000 :  0.7090000000000005\n",
            "9000 :  0.6700000000000005\n",
            "10000 :  0.6780000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJfYhSTYxCm_"
      },
      "source": [
        "It is observed that the performance of the Agent improves with the increase in the number of episodes while the max exploration rate is capped at **0.8**. Also, the decay rate can be changed to 1e-3 to 1e-4 while the learning rate and discount rate are unchanged. These changes give the avg score of 0.75 per thousand episode and the max number of episodes required are greater than hundred thousand. Therefore, we stick to the previously defined parameters since the number of max episodes are only ten thousand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2saR0inafId"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhczDstDaeQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aabefa3-72ee-4bc9-9311-fad019bc1682"
      },
      "source": [
        "# Outer loop resets everything for new episode\n",
        "for episode in range(10):\n",
        "    state = lake.reset()\n",
        "    flag = False\n",
        "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "\n",
        "  # Similar to our training loop, inner loop handles each individual episode\n",
        "    for step in range(steps_per_episode):\n",
        "      clear_output(wait=True)\n",
        "      lake.render()\n",
        "      time.sleep(0.3)\n",
        "\n",
        "      # Taking action from our Q Table\n",
        "      action = np.argmax(q_table[state,:])        \n",
        "      new_state, reward, flag, info = lake.step(action)\n",
        "      print(\"New State:{}\".format(new_state))\n",
        "      time.sleep(0.8)\n",
        "\n",
        "      # Check if the episode is over\n",
        "      if flag:\n",
        "        clear_output(wait=True)\n",
        "        lake.render()\n",
        "        \n",
        "        # reward is one, if the agent reaches the goal\n",
        "        if reward == 1:\n",
        "            print(\"Thanks Programmer! I reached the Goal!!\")\n",
        "            time.sleep(3)\n",
        "        # reward is zero, if the agent fails\n",
        "        else:\n",
        "            print(\"Programmer, you failed to save me!\")\n",
        "            time.sleep(3)\n",
        "            clear_output(wait=True)\n",
        "        break\n",
        "\n",
        "        # state transition\n",
        "        state = new_state\n",
        "\n",
        "# close the environment\n",
        "lake.close()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Programmer, you failed to save me!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DRnxnv4cvun"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-dUP_4cx5X"
      },
      "source": [
        "[Deep Lizard Tutorials](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)"
      ]
    }
  ]
}